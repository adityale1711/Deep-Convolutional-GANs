from __future__ import print_function

import torch
import torch.nn as nn
import torch.utils.data
import torch.nn.parallel
import torch.optim as optim
import torchvision.utils as vutils
import torchvision.datasets as dset
import torchvision.transforms as transforms

from multiprocessing import freeze_support

#Check if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Defining the generator
class G(nn.Module):
    def __init__(self):
        super(G, self).__init__()
        self.main = nn.Sequential(
            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, input):
        output = self.main(input)
        return output

# Defining the discriminator
class D(nn.Module):
    def __init__(self):
        super(D, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        output = self.main(input)
        return output.view(-1)

# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

def trainGANs(imageSize, batchSize, epochs):
    # Creating the transformations
    transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(),
                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])

    # Loading the dataset
    dataset = dset.CIFAR10(root='./data', download=True, transform=transform)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2)

    # Creating the generator
    netG = G().to(device)
    netG.apply(weights_init)

    # Creating the discriminator
    netD = D().to(device)
    netD.apply(weights_init)

    # Training the Deep Convolutional GANs
    criterion = nn.BCELoss()
    optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for i, data in enumerate(dataloader, 0):
            # Updating the weights of the neural network of the discriminator
            netD.zero_grad()

            # Training the discriminator with real image of the dataset
            real, _ = data
            input_real = real.to(device)
            label_real = torch.ones(input_real.size()[0]).to(device)
            output_real = netD(input_real)
            errD_real = criterion(output_real, label_real)
            errD_real.backward()

            # Training the discriminator with a fake image generated by the generator
            noise = torch.randn(input_real.size()[0], 100, 1, 1).to(device)
            fake = netG(noise)
            label_fake = torch.zeros(input_real.size()[0]).to(device)
            output_fake = netD(fake.detach())
            errD_fake = criterion(output_fake, label_fake)
            errD_fake.backward()

            # Update D
            optimizerD.step()

            # Update G network with Feature Matching Loss
            netG.zero_grad()
            label_gen = torch.ones(input_real.size()[0]).to(device)

            # Forward pass fake batch through D again
            output_fake_for_G = netD(fake)

            # Generator loss based on Discriminator's output
            errG_adv = criterion(output_fake_for_G, label_gen)

            # Feature extraction for real and fake
            feature_extractor = nn.Sequential(*list(netD.main.children())[:7])
            with torch.no_grad():
                real_feature = feature_extractor(input_real)
            fake_feature = feature_extractor(fake)

            # Calculate Feature Matching Loss (MSE between real and fake)
            fm_loss = nn.MSELoss()(fake_feature.mean(0), real_feature.mean(0))

            # Back propagating the total error
            errG = errG_adv + fm_loss

            # Updating the weights of the neural network of the generator
            errG.backward()
            optimizerG.step()

            # Print the losses and saving the real images and the generated images
            print(f'[{epoch}/{epochs}][{i}/{len(dataloader)}] Loss_D: {errD_real.item() + errD_fake.item():.4f}, '
                  f'Loss_G: {errG_adv.item():.4f}, FM_Loss: {fm_loss.item():.4f}')
            if i % 100 == 0:
                with torch.no_grad():
                    fake = netG(noise).detach()

                vutils.save_image(input_real, f'./results/real_samples_epochs_{epoch}.png', normalize=True)
                vutils.save_image(fake.data, f'./results/fake_samples_epochs_{epoch}.png', normalize=True)

if __name__ == '__main__':
    freeze_support()
    trainGANs(imageSize=64, batchSize=64, epochs=25)